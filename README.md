# RESTful Booker API Test Automation Framework

A comprehensive API test automation framework for the [RESTful Booker API](https://restful-booker.herokuapp.com) built with Python, pytest, and pytest-bdd. This framework provides robust testing capabilities including functional, security, performance, and negative testing scenarios.

## ğŸš€ Features

- **ğŸ¯ Comprehensive Test Coverage**: Functional, security, performance, and negative testing
- **ğŸ”§ BDD Approach**: Behavior-driven development with Gherkin feature files
- **ğŸ“Š Rich Reporting**: HTML and JSON reports with detailed test analytics
- **ğŸ”„ CI/CD Integration**: GitHub Actions workflow for automated testing
- **ğŸ›¡ï¸ Security Testing**: Built-in security validation and vulnerability checks
- **âš¡ Performance Testing**: Response time validation and performance thresholds
- **ğŸ”€ Parallel Execution**: Support for parallel test execution with pytest-xdist
- **ğŸ“ˆ Advanced Analytics**: Custom reporting with performance metrics
- **ğŸ¨ Modular Architecture**: Clean, maintainable code structure with helper classes
- **ğŸ”§ Configurable**: Environment-specific configurations and flexible test execution

## ğŸ“‹ Table of Contents

- [Project Structure](#-project-structure)
- [Prerequisites](#-prerequisites)
- [Installation](#-installation)
- [Configuration](#-configuration)
- [Usage](#-usage)
- [Test Execution](#-test-execution)
- [CI/CD Integration](#-cicd-integration)
- [Test Categories](#-test-categories)
- [Reporting](#-reporting)
- [Contributing](#-contributing)
- [Troubleshooting](#-troubleshooting)

## ğŸ“ Project Structure

```
restful_booker_automation/
â”œâ”€â”€ ğŸ“‚ config/                     # Configuration files
â”‚   â”œâ”€â”€ config.py                  # Main configuration class
â”‚   â””â”€â”€ environments.json          # Environment-specific settings
â”œâ”€â”€ ğŸ“‚ data/                       # Test data files
â”‚   â”œâ”€â”€ booking_data.json          # Valid booking test data
â”‚   â”œâ”€â”€ invalid_data.json          # Invalid data for negative tests
â”‚   â””â”€â”€ test_schemas.json          # JSON schemas for validation
â”œâ”€â”€ ğŸ“‚ features/                   # BDD feature files (Gherkin)
â”‚   â”œâ”€â”€ auth.feature               # Authentication scenarios
â”‚   â”œâ”€â”€ booking_crud.feature       # CRUD operations
â”‚   â”œâ”€â”€ booking_search.feature     # Search functionality
â”‚   â””â”€â”€ negative_tests.feature     # Negative test scenarios
â”œâ”€â”€ ğŸ“‚ tests/                      # Test implementation
â”‚   â”œâ”€â”€ ğŸ“‚ data/
â”‚   â”‚   â””â”€â”€ booking_factory.py     # Test data factory
â”‚   â”œâ”€â”€ ğŸ“‚ helpers/                # Helper classes
â”‚   â”‚   â”œâ”€â”€ api_helper.py          # Core API operations
â”‚   â”‚   â”œâ”€â”€ auth_helper.py         # Authentication utilities
â”‚   â”‚   â”œâ”€â”€ booking_helper.py      # Booking-specific operations
â”‚   â”‚   â”œâ”€â”€ performance_helper.py  # Performance testing utilities
â”‚   â”‚   â””â”€â”€ security_helper.py     # Security testing utilities
â”‚   â”œâ”€â”€ ğŸ“‚ step_definitions/       # BDD step implementations
â”‚   â”‚   â”œâ”€â”€ test_auth.py           # Authentication step definitions
â”‚   â”‚   â”œâ”€â”€ test_booking_crud.py   # CRUD step definitions
â”‚   â”‚   â”œâ”€â”€ test_booking_search.py # Search step definitions
â”‚   â”‚   â””â”€â”€ test_negative_tests.py # Negative test implementations
â”‚   â””â”€â”€ ğŸ“‚ utils/
â”‚       â””â”€â”€ advanced_reporter.py   # Custom reporting utilities
â”œâ”€â”€ ğŸ“‚ reports/                    # Generated test reports
â”œâ”€â”€ ğŸ“‚ .github/workflows/          # CI/CD workflows
â”‚   â””â”€â”€ api-tests.yml              # GitHub Actions workflow
â”œâ”€â”€ conftest.py                    # pytest configuration and fixtures
â”œâ”€â”€ pytest.ini                     # pytest settings
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ test_runner.py                 # Interactive test runner
â””â”€â”€ README.md                      # This file
```

## ğŸ”§ Prerequisites

- **Python 3.12+** (recommended)
- **pip** (Python package installer)
- **Git** (for version control)
- **Internet connection** (to access RESTful Booker API)

## ğŸ“¦ Installation

### 1. Clone the Repository

```bash
git clone <repository-url>
cd restful_booker_automation
```

### 2. Create Virtual Environment

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# macOS/Linux
python3 -m venv venv
source venv/bin/activate
```

### 3. Install Dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

### 4. Verify Installation

```bash
pytest --version
python test_runner.py
```

## âš™ï¸ Configuration

### Environment Variables

Set the following environment variables (optional):

```bash
# API Configuration
export API_BASE_URL="https://restful-booker.herokuapp.com"
export TEST_ENV="production"
export REQUEST_TIMEOUT="30"
export RETRY_COUNT="3"
export RETRY_BACKOFF="1.5"
```

### Environment-Specific Configuration

Edit `config/environments.json` for environment-specific settings:

```json
{
  "production": {
    "base_url": "https://restful-booker.herokuapp.com",
    "timeout": 30,
    "retry_count": 3,
    "rate_limit_aware": true
  },
  "staging": {
    "base_url": "https://staging-restful-booker.herokuapp.com",
    "timeout": 45,
    "retry_count": 5,
    "rate_limit_aware": false
  }
}
```

## ğŸ¯ Usage

### Interactive Test Runner

The easiest way to run tests is using the interactive test runner:

```bash
python test_runner.py
```

This will present a menu with the following options:

```
Available test suites:
 1. Smoke Tests
 2. Functional Tests
 3. Non-Functional Tests
 4. Positive Tests
 5. Negative Tests
 6. Auth Module Only
 7. CRUD Module Only
 8. Search Module Only
 9. All Tests
```

### Command Line Execution

You can also run tests directly with pytest:

```bash
# Run all tests
pytest -v

# Run specific test categories
pytest -m smoke -v                    # Smoke tests
pytest -m functional -v               # Functional tests
pytest -m security -v                 # Security tests
pytest -m negative -v                 # Negative tests
pytest -m positive -v                 # Positive tests

# Run specific modules
pytest tests/step_definitions/test_auth.py -v
pytest tests/step_definitions/test_booking_crud.py -v
pytest tests/step_definitions/test_booking_search.py -v

# Run with parallel execution
pytest -n auto -v                     # Auto-detect CPU cores
pytest -n 4 -v                        # Use 4 parallel workers

# Generate custom reports
pytest --html=reports/custom_report.html --self-contained-html -v
```

## ğŸ§ª Test Execution

### Test Categories and Markers

The framework uses pytest markers to categorize tests:

| Marker        | Description                  | Usage                   |
| ------------- | ---------------------------- | ----------------------- |
| `smoke`       | Critical functionality tests | `pytest -m smoke`       |
| `functional`  | Feature testing              | `pytest -m functional`  |
| `security`    | Security validation          | `pytest -m security`    |
| `performance` | Response time validation     | `pytest -m performance` |
| `negative`    | Error condition testing      | `pytest -m negative`    |
| `positive`    | Happy path scenarios         | `pytest -m positive`    |
| `auth`        | Authentication tests         | `pytest -m auth`        |
| `booking`     | Booking operations           | `pytest -m booking`     |
| `crud`        | CRUD operations              | `pytest -m crud`        |
| `search`      | Search functionality         | `pytest -m search`      |

### Advanced Test Execution

```bash
# Run tests with specific markers combination
pytest -m "smoke and positive" -v
pytest -m "functional and not security" -v

# Run tests with custom timeout
pytest --timeout=60 -v

# Run tests with detailed output
pytest -v --tb=long --capture=no

# Run tests and stop on first failure
pytest -x -v

# Run tests with coverage
pytest --cov=tests --cov-report=html -v
```

## ğŸ”„ CI/CD Integration

### GitHub Actions Workflow

The project includes a comprehensive GitHub Actions workflow (`.github/workflows/api-tests.yml`) that provides:

- **Manual Trigger**: Workflow dispatch with configurable options
- **Environment Selection**: Choose test environment (production, staging)
- **Test Suite Selection**: Run specific test suites or all tests
- **Artifact Generation**: Automatic report generation and upload
- **Detailed Reporting**: Test summary in GitHub Actions UI

#### Workflow Features:

- âœ… **Flexible Execution**: Choose environment and test suite
- ğŸ“Š **Rich Reporting**: HTML and JSON reports with test metrics
- ğŸ”„ **Retry Logic**: Built-in retry for flaky tests
- ğŸ“ˆ **Performance Tracking**: Duration and success rate metrics
- ğŸ¯ **Targeted Testing**: Run specific modules or test categories
- ğŸ“¤ **Artifact Upload**: Reports stored for 30 days

#### Manual Workflow Trigger:

1. Go to **Actions** tab in GitHub repository
2. Select **RESTful Booker API Test Automation** workflow
3. Click **Run workflow**
4. Configure options:
   - **Environment**: production
   - **Test Suite**: Choose from available options
   - **Generate Artifacts**: Enable/disable report generation

### Local CI Simulation

```bash
# Simulate CI environment locally
export TEST_ENV=production
export API_BASE_URL=https://restful-booker.herokuapp.com
pytest -v --html=reports/ci_report.html --json-report --json-report-file=reports/ci_report.json
```

## ğŸ“Š Test Categories

### 1. Authentication Tests (`auth.feature`)

- âœ… Valid credential authentication
- âŒ Invalid credential handling
- ğŸ”’ Token validation and security
- ğŸš« Empty credential scenarios

### 2. CRUD Operations (`booking_crud.feature`)

- â• **Create**: New booking creation with validation
- ğŸ“– **Read**: Booking retrieval and data verification
- âœï¸ **Update**: Full (PUT) and partial (PATCH) updates
- ğŸ—‘ï¸ **Delete**: Booking deletion and cleanup
- ğŸ”„ **Workflow**: Complete CRUD lifecycle testing

### 3. Search Operations (`booking_search.feature`)

- ğŸ” Search by various criteria
- ğŸ“… Date-based filtering
- ğŸ‘¤ Name-based searches
- ğŸ’° Price range filtering

### 4. Negative Testing (`negative_tests.feature`)

- ğŸš« Invalid data handling
- ğŸ”¢ Boundary value testing
- ğŸ›¡ï¸ Security vulnerability testing
- âš ï¸ Error response validation

## ğŸ“ˆ Reporting

### Report Types

The framework generates multiple report formats:

#### 1. HTML Reports

- **Location**: `reports/`
- **Features**: Interactive, self-contained, detailed test results
- **Naming**: `{suite_name}_{timestamp}_report.html`

#### 2. JSON Reports

- **Location**: `reports/`
- **Features**: Machine-readable, integration-friendly
- **Naming**: `{suite_name}_{timestamp}_report.json`

#### 3. Console Output

- **Real-time**: Live test execution feedback
- **Structured**: Organized by test categories
- **Colored**: Success/failure indicators

### Report Analysis

```bash
# View latest HTML report
# Open reports/{latest}_report.html in browser

# Analyze JSON report programmatically
python -c "
import json
with open('reports/{latest}_report.json') as f:
    data = json.load(f)
    print(f'Total: {data[\"summary\"][\"total\"]}')
    print(f'Passed: {data[\"summary\"][\"passed\"]}')
    print(f'Failed: {data[\"summary\"][\"failed\"]}')
"
```

### Custom Reporting

The framework includes advanced reporting capabilities:

```python
# tests/utils/advanced_reporter.py
# Custom metrics and analytics
# Performance trend analysis
# Failure pattern detection
```

## ğŸ› ï¸ Development

### Adding New Tests

1. **Create Feature File** (BDD approach):

```gherkin
# features/new_feature.feature
Feature: New Feature
    Scenario: Test scenario
        Given precondition
        When action
        Then expected result
```

2. **Implement Step Definitions**:

```python
# tests/step_definitions/test_new_feature.py
from pytest_bdd import scenarios, given, when, then

scenarios('../features/new_feature.feature')

@given('precondition')
def precondition():
    pass

@when('action')
def action():
    pass

@then('expected result')
def expected_result():
    pass
```

3. **Add Test Markers**:

```python
@pytest.mark.smoke
@pytest.mark.positive
def test_function():
    pass
```

### Helper Classes

Extend functionality using helper classes:

```python
# tests/helpers/custom_helper.py
class CustomHelper:
    def __init__(self, api_helper):
        self.api = api_helper

    def custom_operation(self):
        # Implementation
        pass
```

## ğŸ”§ Troubleshooting

### Common Issues

#### 1. **Import Errors**

```bash
# Ensure virtual environment is activated
source venv/bin/activate  # macOS/Linux
venv\Scripts\activate     # Windows

# Reinstall dependencies
pip install -r requirements.txt
```

#### 2. **API Connection Issues**

```bash
# Check API availability
curl https://restful-booker.herokuapp.com/ping

# Verify environment variables
echo $API_BASE_URL
```

#### 3. **Test Failures**

```bash
# Run with verbose output
pytest -v --tb=long

# Run single test for debugging
pytest tests/step_definitions/test_auth.py::test_specific_function -v -s
```

#### 4. **Report Generation Issues**

```bash
# Ensure reports directory exists
mkdir -p reports

# Check permissions
ls -la reports/
```

### Performance Optimization

```bash
# Use parallel execution
pytest -n auto

# Skip slow tests during development
pytest -m "not performance"

# Use faster assertion introspection
pytest --tb=no
```

### Debugging

```python
# Add debug logging
import logging
logging.basicConfig(level=logging.DEBUG)

# Use pytest debugging
pytest --pdb  # Drop into debugger on failure
pytest --pdbcls=IPython.terminal.debugger:Pdb  # Use IPython debugger
```

## ğŸ“ Best Practices

### 1. **Test Organization**

- Group related tests in feature files
- Use descriptive scenario names
- Maintain clear test data separation

### 2. **Data Management**

- Use factories for dynamic test data
- Keep static test data in JSON files
- Implement proper test cleanup

### 3. **Error Handling**

- Implement robust retry mechanisms
- Use appropriate timeouts
- Handle network failures gracefully

### 4. **Reporting**

- Generate reports for every test run
- Include performance metrics
- Maintain historical test data
